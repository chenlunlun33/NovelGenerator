{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FactorizedEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_sequence_length=300):\n",
    "        super(FactorizedEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embedded = self.embedding(input)\n",
    "        return embedded\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Self-attention\n",
    "        residual = x\n",
    "        x, _ = self.self_attn(x, x, x)\n",
    "        x = residual + self.dropout(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Feed-forward\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, hidden_dim, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class StochasticDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout):\n",
    "        super(StochasticDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        # Self-attention\n",
    "        residual = x\n",
    "        x, _ = self.self_attn(x, x, x)\n",
    "        x = residual + self.dropout(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Cross-attention\n",
    "        residual = x\n",
    "        x, _ = self.cross_attn(x, encoder_output, encoder_output)\n",
    "        x = residual + self.dropout(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Feed-forward + stochastic\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        x = residual + torch.randn_like(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, tgt_mask=None):\n",
    "        # Self-attention\n",
    "        residual = x\n",
    "        attn_output, _ = self.self_attn(x, x, x, key_padding_mask=tgt_mask)\n",
    "        x = residual + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Cross-attention\n",
    "        residual = x\n",
    "        cross_attn_output, _ = self.cross_attn(x, encoder_output, encoder_output)\n",
    "        x = residual + self.dropout(cross_attn_output)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Feed-forward\n",
    "        residual = x\n",
    "        x = self.ffn(x)\n",
    "        x = residual + self.dropout(x)\n",
    "        x = self.norm3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DecoderStochastic(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, hidden_dim, dropout, max_output_length):\n",
    "        super(DecoderStochastic, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            StochasticDecoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.max_output_length = max_output_length\n",
    "    \n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output)\n",
    "            x = x[:, :self.max_output_length]\n",
    "        return x\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, hidden_dim, dropout, max_output_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output)\n",
    "            \n",
    "            # 截斷長度到 self.max_output_length\n",
    "            x = x[:, :self.max_output_length]\n",
    "            \n",
    "            # 填充\n",
    "            padding_length = self.max_output_length - x.size(1)\n",
    "            if padding_length > 0:\n",
    "                padding = x.new_zeros(x.size(0), padding_length, x.size(2))\n",
    "                x = torch.cat((x, padding), dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, word_embedd_size, encoder_embed_dim, decoder_embed_dim, num_encoder_layers, num_decoder_layers, num_heads, hidden_dim, dropout, max_output_length):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding_encoder = FactorizedEmbedding(word_embedd_size, encoder_embed_dim)\n",
    "        self.embedding_decoder = FactorizedEmbedding(word_embedd_size, decoder_embed_dim)\n",
    "        self.encoder = Encoder(num_encoder_layers, encoder_embed_dim, num_heads, hidden_dim, dropout)\n",
    "        self.decoder_stochastic = DecoderStochastic(num_decoder_layers-12, decoder_embed_dim, num_heads, hidden_dim, dropout, max_output_length)\n",
    "        self.decoder_1 = Decoder(6, decoder_embed_dim, num_heads, hidden_dim, dropout, max_output_length)\n",
    "        self.decoder_2 = Decoder(6, decoder_embed_dim, num_heads, hidden_dim, dropout, max_output_length)\n",
    "        self.fc = nn.Linear(decoder_embed_dim, word_embedd_size)\n",
    "    \n",
    "    def forward(self, src, tgt=None, tgt_mask=None, pad_mask=None):\n",
    "        src = F.pad(src, (0, 50 - src.size(1)), value=0)\n",
    "        encoder_embedded = self.embedding_encoder(src)\n",
    "        encoder_output = self.encoder(encoder_embedded)\n",
    "        \n",
    "        if tgt is not None:\n",
    "            tgt = F.pad(tgt, (0, 50 - tgt.size(1)), value=0)\n",
    "            decoder_embedded = self.embedding_decoder(tgt)\n",
    "            decoder_output = self.decoder_1(decoder_embedded, encoder_output)\n",
    "            decoder_output = self.decoder_stochastic(decoder_embedded, decoder_output)\n",
    "            decoder_output = self.decoder_2(decoder_embedded, decoder_output)\n",
    "            # 用softmax做機率分布輸出\n",
    "            output = self.fc(decoder_output)\n",
    "            output = nn.functional.softmax(output, dim=-1)\n",
    "        \n",
    "            if tgt_mask is not None:\n",
    "                tgt_mask = tgt_mask.unsqueeze(2)\n",
    "                output = output.masked_fill(tgt_mask == 0, float('-inf'))\n",
    "            if pad_mask is not None:\n",
    "                pad_mask = pad_mask.unsqueeze(2)\n",
    "                output = output.masked_fill(pad_mask.unsqueeze(2) == 0, float('-inf'))\n",
    "        \n",
    "        else:\n",
    "            # 從起始token，然後逐步生成下一個token，直到生成結束或達到最大長度\n",
    "            \n",
    "            generated_tokens = []  # 用於儲存生成出來的token\n",
    "            max_length = 50  # 最大生成長度\n",
    "            start_token = 0\n",
    "\n",
    "            # 初始化生成序列，將起始token加到生成序列中\n",
    "            current_token = start_token\n",
    "            generated_tokens.append(current_token)\n",
    "\n",
    "            # 逐步生成下一個token，直到達到最大長度或生成結束token\n",
    "            while len(generated_tokens) < max_length and current_token != 0:\n",
    "                # 使用模型解碼器來預測下一個token\n",
    "                decoder_input = torch.tensor([generated_tokens[-1]], dtype=torch.long)\n",
    "                decoder_embedded = self.embedding_decoder(decoder_input)\n",
    "                \n",
    "                # 不同的解碼層\n",
    "                decoder_output_1 = self.decoder_1(decoder_embedded, encoder_output)\n",
    "                decoder_output_stochastic = self.decoder_stochastic(decoder_embedded, decoder_output_1)\n",
    "                decoder_output_2 = self.decoder_2(decoder_embedded, decoder_output_stochastic)\n",
    "                \n",
    "                # 用softmax做機率分布輸出\n",
    "                output = self.fc(decoder_output_2)\n",
    "                output = nn.functional.softmax(output, dim=-1)\n",
    "                \n",
    "                if tgt_mask is not None:\n",
    "                    tgt_mask = tgt_mask.unsqueeze(2)\n",
    "                    output = output.masked_fill(tgt_mask == 0, float('-inf'))\n",
    "\n",
    "                # 取出最後一個時間步的機率分佈\n",
    "                next_token_probs = output[0, -1, :]\n",
    "                # 選擇最高機率token\n",
    "                next_token = torch.argmax(next_token_probs).item()\n",
    "                \n",
    "                generated_tokens.append(next_token)\n",
    "                \n",
    "                # 是否生成ending token\n",
    "                if next_token == 0:\n",
    "                    break\n",
    "\n",
    "            return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "\n",
    "def load_vocab(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        vocab = file.read().splitlines()\n",
    "    return vocab\n",
    "\n",
    "def Trans_text(text, vocab, unk_token='2'):\n",
    "    decoded_text = []\n",
    "    for word in text:\n",
    "        if word in vocab:\n",
    "            decoded_text.append(vocab.index(word))\n",
    "        elif word == \" \":\n",
    "            decoded_text.append(1)\n",
    "        else:\n",
    "            decoded_text.append(2)\n",
    "    return np.array(decoded_text, dtype=np.int64).flatten()\n",
    "\n",
    "def encoder_input(src, encoder_vocab_path):\n",
    "\n",
    "    encoder_vocab = load_vocab(encoder_vocab_path)\n",
    "    encoder_encoded = Trans_text(src, encoder_vocab)\n",
    "\n",
    "    return encoder_encoded\n",
    "\n",
    "def decoder_input(tgt, decoder_vocab_path):\n",
    "    \n",
    "    decoder_vocab = load_vocab(decoder_vocab_path)\n",
    "    decoder_encoded = Trans_text(tgt, decoder_vocab)\n",
    "\n",
    "    return decoder_encoded\n",
    "\n",
    "# 小說數據集\n",
    "class NovelDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        text = text + \"|\"\n",
    "        self.textLen = len(text)\n",
    "        self.count = len(text)//49\n",
    "        self.src_text = text[:self.count*49]\n",
    "        self.tgt_text = text[49:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.count - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_data = self.src_text[idx*49 : (idx+1)*49] + '§'\n",
    "        # '|' == 開頭結尾符號\n",
    "        tgt = '|' + self.tgt_text[idx*49 : (idx+1)*49]\n",
    "        # tokenizer\n",
    "        input_tokens = encoder_input(src_data, embedd_vocab_path)\n",
    "        target_tokens = decoder_input(tgt, embedd_vocab_path)\n",
    "\n",
    "        input_tokens = np.array(input_tokens, dtype=np.int64)\n",
    "        target_tokens = np.array(target_tokens, dtype=np.int64)\n",
    "        \n",
    "        return input_tokens, target_tokens\n",
    "\n",
    "def collate_fn(data):\n",
    "    input_tokens, target_tokens = zip(*data)\n",
    "\n",
    "    # 轉換tensor\n",
    "    input_tokens = [torch.tensor(tokens, dtype=torch.long) for tokens in input_tokens]\n",
    "    target_tokens = [torch.tensor(tokens, dtype=torch.long) for tokens in target_tokens]\n",
    "\n",
    "    # 填充序列\n",
    "    padded_input_tokens = pad_sequence(input_tokens, batch_first=True)\n",
    "    padded_target_tokens = pad_sequence(target_tokens, batch_first=True)\n",
    "\n",
    "    # pad_mask\n",
    "    # PAD_TOKEN = 0\n",
    "    # pad_mask = (padded_target_tokens != PAD_TOKEN)\n",
    "    \n",
    "    return padded_input_tokens, padded_target_tokens#, pad_mask\n",
    "\n",
    "def create_tgt_mask(tgt):\n",
    "    # 建立與目標序列相同形狀的全是零的張量\n",
    "    tgt_mask = torch.zeros_like(tgt, dtype=torch.bool)\n",
    "\n",
    "    # 在序列的上三角型部分設為True，表示不能看到未来資訊\n",
    "    seq_len = tgt.size(1)\n",
    "    tgt_mask[:, 1:seq_len] = torch.triu(torch.ones(1, seq_len-1, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    return tgt_mask\n",
    "\n",
    "# 繪製loss的折線圖\n",
    "def plot_loss(loss_values):\n",
    "    plt.plot(loss_values, label='Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數初始化\n",
    "\n",
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         nn.init.xavier_uniform_(m.weight.data)\n",
    "#         if m.bias is not None:\n",
    "#             nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 模型參數\n",
    "word_embedd_size = 9713\n",
    "encoder_embed_dim = 768\n",
    "decoder_embed_dim = 768\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 18\n",
    "num_heads = 16\n",
    "hidden_dim = 1024\n",
    "dropout = 0.1\n",
    "max_output_length = 50\n",
    "\n",
    "# 模型和scaler\n",
    "model = Transformer(word_embedd_size, encoder_embed_dim, decoder_embed_dim, num_encoder_layers, num_decoder_layers, num_heads, hidden_dim, dropout, max_output_length)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 設置訓練環境\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 訓練參數、optimizer、criterion\n",
    "embedd_vocab_path = \"vocab\\\\vocabularyDel.txt\"\n",
    "batch_size = 4\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0005\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss_values = []\n",
    "\n",
    "# 獲得指定資料夾裡所有檔案路徑\n",
    "folder_path = 'WebNovelTxt'\n",
    "file_paths_list = []\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.relpath(os.path.join(root, file), folder_path)\n",
    "        file_paths_list.append(file_path)\n",
    "        \n",
    "step = 0\n",
    "\n",
    "# 訓練\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # 載入資料\n",
    "    for i in file_paths_list:\n",
    "        with open('WebNovelTxt\\\\' + i, 'r', encoding='utf-8') as file:\n",
    "            novel_data = file.read()\n",
    "\n",
    "        # 創建Dataset、DataLoader\n",
    "        dataset = NovelDataset(novel_data)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        \n",
    "        # for input_ids, target_ids, pad_mask in dataloader:\n",
    "        for input_ids, target_ids in dataloader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            # one hot encode\n",
    "            target = F.one_hot(target_ids, num_classes=word_embedd_size)\n",
    "            target = target.type(torch.float64)\n",
    "            target_ids = target_ids.to(device)\n",
    "            # tgt_mask生成\n",
    "            tgt_mask = create_tgt_mask(target)\n",
    "            tgt_mask = tgt_mask.to(device)\n",
    "            # pad_mask = pad_mask.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # 清除梯度值\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 降低運算記憶體\n",
    "            with autocast():\n",
    "                \n",
    "                # 前向傳播\n",
    "                output = model(input_ids, target_ids, tgt_mask=tgt_mask, pad_mask=None)\n",
    "                \n",
    "                # 計算損失\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            # 梯度裁剪\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        step += 1\n",
    "        if step % 10 == 0:\n",
    "            torch.save(model.state_dict(), 'save\\\\step' + str(step) + 'state_dict.pt')\n",
    "    torch.save(model.state_dict(), 'save\\\\' + str(epoch) + 'state_dict.pt')\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    loss_values.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 繪製loss的摺線圖\n",
    "plot_loss(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入儲存好模型參數\n",
    "\n",
    "# 模型參數\n",
    "word_embedd_size = 9713\n",
    "encoder_embed_dim = 768\n",
    "decoder_embed_dim = 768\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 18\n",
    "num_heads = 16\n",
    "hidden_dim = 1024\n",
    "dropout = 0.1\n",
    "max_output_length = 50\n",
    "embedd_vocab_path = \"vocab\\\\vocabularyDel.txt\"\n",
    "\n",
    "# 建模型\n",
    "model = Transformer(word_embedd_size, encoder_embed_dim, decoder_embed_dim, num_encoder_layers, num_decoder_layers, num_heads, hidden_dim, dropout, max_output_length)\n",
    "model.load_state_dict(torch.load('step700000state_dict.pt'))\n",
    "\n",
    "# 設置訓練環境\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入已訓練的模型權重\n",
    "# model.load_state_dict(torch.load('trained_model.pth'))\n",
    "\n",
    "# 評估模式\n",
    "model.eval()\n",
    "\n",
    "# 讀取字彙表txt\n",
    "vocab_file = \"vocab\\\\vocabularyDel.txt\"\n",
    "\n",
    "# 創建字彙表對應token的字典\n",
    "vocab = {}\n",
    "with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for index, line in enumerate(lines):\n",
    "        # 移除換行符\n",
    "        line = line.strip()\n",
    "        token = str(index)\n",
    "        word = line\n",
    "        vocab[token] = word\n",
    "\n",
    "# 範例句\n",
    "input_word = '我是範例句'\n",
    "\n",
    "# 轉成token\n",
    "input_tokens = encoder_input(input_word, embedd_vocab_path)\n",
    "input_tokens = np.array(input_tokens, dtype=np.int64)\n",
    "src = torch.tensor(input_tokens, dtype=torch.long)\n",
    "src = src.reshape(1, -1)\n",
    "\n",
    "# 需要的輸入長度\n",
    "desired_seq_length = 50\n",
    "\n",
    "# 檢查輸入的長度，太長截斷，太短填充\n",
    "if src.shape[1] < desired_seq_length:\n",
    "    # 如果輸入長度太短，進行填充\n",
    "    padding_length = desired_seq_length - src.shape[1]\n",
    "    src = F.pad(src, (0, padding_length), value=0)\n",
    "elif src.shape[1] > desired_seq_length:\n",
    "    # 如果輸入長度太長，進行截斷\n",
    "    src = src[:, -desired_seq_length:]\n",
    "\n",
    "src = src.to(device)\n",
    "generate_length = 50  # 自定義生成的最大長度\n",
    "\n",
    "# 起始tgt token\n",
    "start_token = torch.tensor([[0]], dtype=torch.long).to(device)\n",
    "\n",
    "# 生成的token sequence\n",
    "generated_sequence = [start_token]\n",
    "\n",
    "while len(generated_sequence) < generate_length:\n",
    "    with torch.no_grad():\n",
    "        # 處理生成的token序列\n",
    "        decoder_input = generated_sequence[-1].long().to(device)\n",
    "        generated_tokens = model.forward(src, tgt=decoder_input)\n",
    "\n",
    "        # 處理模型的輸出預測結果\n",
    "        # 將模型預測的下個token添加到generated_sequence\n",
    "        next_token_probs = generated_tokens[:, -1, :]\n",
    "        next_token = torch.multinomial(next_token_probs, 1)\n",
    "        generated_sequence.append(next_token)\n",
    "        # 遇到結束符號停止生成\n",
    "        if next_token.item() == 0:\n",
    "            break\n",
    "\n",
    "# 將生成的token，轉為文本輸出\n",
    "generated_text = ''\n",
    "for token in generated_sequence:\n",
    "    newWord = vocab[str(token.item())]\n",
    "    generated_text += newWord\n",
    "print('------------------------------文章------------------------------')\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python PyTorch",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
